{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02205ef0-b53d-4df8-8596-1eb2d0790b23",
   "metadata": {},
   "source": [
    "## Probability, entropy, and Kullback-Leiber divergence\n",
    "\n",
    "Images are made from pixels which are dotss on a screen - an image is formed when pixels are arranged in a specific pattern. Image resolution is determined by number of pixels contained in an image.\n",
    "\n",
    "There is a directly proportional relationship between number of pixels in an image and the quality of an image. \n",
    "\n",
    "Image generation occurs by using the pattern or probability of an image's pixels distributed in a space. \n",
    "\n",
    "### Probability and Shannon entropy\n",
    "A fair coin has two possible outcomes - head or tails - with both having 50\\% probability of occurring.  To model the probability of a coin system:\n",
    "\n",
    "- we can represent the outcome of a coin as a random variable X (discrete) with a probability mass distribution $P(X) = \\{0.5;0.5\\}$\n",
    "- if we have an unfair coin with a PMD of $p(X) = \\{0.65;0.35\\}$ and toss it n times, we cann calculate the probability of getting outcome x for y number of times using a binomial distribution function.\n",
    "\n",
    "**Shanon  Entropy of Information** Measures the amount of information contained in a random variable or the minimum bits required to encode information of a system.\n",
    "\n",
    "$$\n",
    "H(P) = - \\sum_{x \\in \\mathcal{X}} P(x) \\log P(x)\n",
    "$$\n",
    "\n",
    "**Kullback-Leibler divergence and cross entropy**\n",
    "\n",
    "$D_{KL}$  is the expected value of the log likelihood ratio of two distributions of the same random variable. This ratio measres the difference between two PDFs of a system with the same random variable.\n",
    "\n",
    "$$\n",
    "D_{\\mathrm{KL}}(P \\parallel Q) = \\sum_{x \\in \\mathcal{X}} P(x) \\ln \\frac{P(x)}{Q(x)}\n",
    "$$\n",
    "\n",
    "**Cross entropy** - another ratio used to quantify difference between 2 PDFs and is normally used as a loss function in machine  learning.\n",
    "\n",
    "$$\n",
    "[\n",
    "H(p, q) = H(p) + D_{\\mathrm{KL}}(p \\| q)\n",
    "]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2732e82-2ea1-40d8-a878-0669db34a8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "726e7c2a-e2c9-4579-8a74-2895e551b201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binomial_distribution_function(p,x,n):\n",
    "    combination = math.factorial(n)/(math.factorial(x)*math.factorial(n-x))\n",
    "    prob_success = p**x\n",
    "    prob_fail = (1-p)**(n-x)\n",
    "    probability = combination*prob_success*prob_fail\n",
    "    return probability \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56ce5b30-61ec-4dbf-855a-f1ae84c1ff9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_distribution_function(x,mean,variance):\n",
    "    ratio = 1 / math.sqrt(2*math.pi*variance)\n",
    "    z_score = (x-mean)/ math.sqrt(variance)\n",
    "    probability = ratio * math.e ** (-0.5*z_score**2)\n",
    "    return probability\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e5a595-0d14-4711-a962-66b29a5e4a19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8f751505-9f48-4fca-b8e2-9a9ecd1f1ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shannon_entropy(p_list):\n",
    "    probability = 0\n",
    "    for prob in p_list:\n",
    "        info = prob * math.log(prob)\n",
    "        probability += info\n",
    "    entropy= -probability\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "24e00866-9f25-4b04-a5ec-2ca8c158946c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_kullback_leibler_divergence(target_pmd, real_pmd):\n",
    "    if len(target_pmd) != len(real_pmd):\n",
    "        raise ValueError(\"length target and real pmds do not match\")\n",
    "    k_l_divergence = 0\n",
    "    for i in range(len(target_pmd)):\n",
    "        log_likelihood_ratio = target_pmd[i] * math.log(target_pmd[i]/real_pmd[i])\n",
    "        k_l_divergence += log_likelihood_ratio\n",
    "    return k_l_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "54c777cf-0a9e-4a89-9682-f8bc3c1e259e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23836664662207044\n"
     ]
    }
   ],
   "source": [
    "# P(X) = {0.45;0.35}\n",
    "# n = 10 tosses\n",
    "# x = 4 heads\n",
    "\n",
    "p_4_heads = binomial_distribution_function(0.45,4,10)\n",
    "print(p_4_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0e18c847-7a90-4b96-a216-36b13b84cde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6931471805599453\n"
     ]
    }
   ],
   "source": [
    "entropy = shannon_entropy([0.5,0.5])\n",
    "print(entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "05b2580a-72e3-47a3-834a-74d1ae4285d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04715533973562064\n"
     ]
    }
   ],
   "source": [
    "# Example Scenario\n",
    "# Target distribution of coin - P(X) -> 0.5, 0.5\n",
    "# Real distribution of coin after 10,000 rolls P(X) -> 0.65, 0.35\n",
    "dkl = calc_kullback_leibler_divergence([0.5,0.5],[0.65,0.35])\n",
    "print(dkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "82d6cb1f-d644-413a-85e9-fa776a936006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.740302520295566\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = entropy+dkl\n",
    "print(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4a77aa-da07-4fc0-9926-543f975dc591",
   "metadata": {},
   "source": [
    "## Conditional probabilites and joint entropies\n",
    "\n",
    "A joint PDF is the combination of the multiple random variables which have their own PDFs. \n",
    "According to Bayes' Theorem, $$p(x,y), p(y|x), p(x), and p(y)$$\n",
    "\n",
    "$$\n",
    "P( y\\mid x) = \\frac{P(x \\mid y) \\, P( y)}{P( x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86cd119-83f1-4005-bec6-11d5e30ebb2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
